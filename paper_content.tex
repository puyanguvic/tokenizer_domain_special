
% =========================
% Abstract (ICML: single paragraph, 4--6 sentences)
% =========================
\begin{abstract}
Tokenization is a performance-critical interface in modern machine learning systems, yet mainstream tokenizers are optimized for natural language and implicitly assume unstructured text.
In structured and semi-structured detection workloads (e.g., protocol messages, HTTP requests, configuration files, logs, and email artifacts), these assumptions fail: subword tokenizers fragment structural units and distort task semantics, while byte-level tokenization preserves structure at prohibitive sequence length and system cost. We propose a tokenizer \emph{system} that formulates tokenization as \emph{controlled compression} and compiles the resulting rules into deterministic finite-state transducers for predictable linear-time execution. To quantify semantic distortion without requiring exact reconstruction, we introduce a \emph{directed semantic distortion} metric defined via teacher--student KL (cross-entropy) between semantics conditioned on raw inputs and the best predictor conditioned only on tokenized inputs, and connect distortion to causal dependency disruption and degraded representation geometry relevant to encoder-only detection. Across multiple structured domains, controlled tokenization reduces token counts under matched vocabulary budgets while improving semantic predictability, yielding higher throughput and lower memory usage, and matching or improving downstream detection/classification performance under equal compute.
\end{abstract}

% =========================
% 1. Introduction (rewritten for a tighter ICML story)
% =========================
\section{Introduction}
\label{sec:intro}

Tokenization is the discrete interface between raw symbolic inputs and sequence models.
By fixing the atomic units of representation, it directly controls sequence length,
system cost, and which regularities are exposed to learning.
In structured and semi-structured detection workloads---protocol messages, HTTP requests,
configuration files, logs, and email/HTML artifacts---this interface is often the
dominant bottleneck: segmentation choices can inflate token length before learning
begins and can also disturb structure-driven semantics that detection relies on.

Mainstream tokenizers (BPE/Unigram) were optimized for natural language.
On structured streams, they frequently produce tokens that cross structural boundaries
(e.g., key--delimiter--value or tag/attribute splits), creating units that blur
task-relevant semantics.
Byte/character-level alternatives avoid boundary violations, but at prohibitive sequence
length, leading to higher latency, lower throughput, and tighter memory ceilings for
encoder-only detectors.
As a result, practitioners face an unfavorable choice between (i) subword tokenization
that is shorter but semantically misaligned, and (ii) raw-unit tokenization that is
aligned but too long for budgeted deployment.

This paper takes a detection-centric view: tokenization should be designed as
\emph{controlled compression}.
The goal is to reduce the expected token length while preserving detection-relevant
predictability available from the raw input.
Crucially, we do \emph{not} require exact reconstruction of the raw string.
Instead, we quantify semantic loss at the interface with a \emph{directed semantic
distortion} defined via a teacher--student KL (equivalently, teacher-aligned
cross-entropy): how much less predictable the task semantics become when a predictor can
only observe tokenized prefixes rather than raw prefixes.
This distortion is directed/prefix-based to match online structure-driven semantics and
is directly actionable for tokenizer design.

We propose \textbf{CTok}, a deployable tokenizer \emph{system} that optimizes a
rate--distortion objective tailored to encoder-only detection.
CTok uses lightweight probes to estimate directed semantic distortion and performs
gain--distortion vocabulary induction under a fixed vocabulary budget.
To make tokenization a reliable system component, CTok compiles the learned rules into a
deterministic finite-state matcher (FST/DFA-equivalent) for predictable linear-time
runtime behavior, reproducibility, and versionable artifacts.

Our experiments in three structured domains (HTTP/WAF, phishing HTML, and HDFS logs)
support a concise story:
(1) CTok improves the rate--distortion frontier under matched vocabulary budgets;
(2) under equal training compute, CTok matches or improves detection while using fewer
tokens, improving throughput and memory; and
(3) CTok yields a more stable interface under semantics-preserving perturbations,
reducing boundary sensitivity and prediction/embedding drift.

\paragraph{Contributions.}
\begin{enumerate}
  \item \textbf{Objective:} We formulate tokenization for encoder-only detection as
  \emph{controlled compression}, minimizing token rate while controlling a task-aligned
  \emph{directed semantic distortion} defined by teacher--student KL on prefixes (rather
  than reconstruction).
  \item \textbf{Method \& system:} We introduce CTok, which estimates directed distortion
  with lightweight probes, performs gain--distortion vocabulary induction, and compiles
  the result into a deterministic finite-state runtime matcher with predictable linear-time
  execution.
  \item \textbf{Evidence:} Across structured detection domains, we show that CTok
  improves the rate--distortion frontier, delivers equal-compute detection gains with
  shorter sequences and better system efficiency, and reduces boundary sensitivity under
  semantics-preserving format drift.
\end{enumerate}

% =========================
% 2. Controlled Compression Objective (merge Background + Formulation)
% =========================
\section{Controlled Compression Objective}
\label{sec:objective}

Let $X\in\Sigma^\ast$ be a structured/semi-structured input string with detection
semantics $Y$ (e.g., a task label) or a prefix-aligned semantic sequence $Y_{1:T}$.
A tokenizer $\tau$ maps $X$ to a token sequence $Z=\tau(X)\in\mathcal{V}^\ast$ over a
finite vocabulary $\mathcal{V}$.

\paragraph{Rate (system cost).}
We measure the \emph{rate} of a tokenizer by the expected token length:
\begin{equation}
R(\tau) \triangleq \mathbb{E}[|Z|],\qquad Z=\tau(X).
\label{eq:rate_main}
\end{equation}
Since encoder-only models incur at least linear cost in $|Z|$ (and quadratic attention
cost in transformers), reducing $R(\tau)$ directly improves throughput and memory.

\paragraph{Directed semantic distortion (task-aligned fidelity).}
We quantify how much tokenization degrades the causal predictability of semantics from
prefixes.
Let $P^\star$ be a teacher distribution capturing semantics from raw prefixes and let a
student predictor observe only tokenized prefixes.
We define directed distortion as the best-achievable teacher--student KL on prefixes:
\begin{equation}
\Delta_{\rightarrow}(\tau)
\triangleq
\min_{q\in\mathcal{Q}}
\ \mathbb{E}\Bigg[
\frac{1}{T}\sum_{t=1}^T
\mathrm{KL}\!\left(P^\star(Y_t\mid X_{1:t})\ \Vert\ q(Y_t\mid Z_{1:t})\right)
\Bigg],
\quad Z=\tau(X).
\label{eq:dist_directed_main}
\end{equation}
This distortion does not demand reconstructing $X$; it measures irrecoverable semantic
uncertainty introduced by the tokenization interface.
(Information-theoretic identities and properties are provided in Appendix~\ref{app:theory}.)

\paragraph{Controlled compression.}
We design tokenizers by trading off rate and directed semantic distortion:
\begin{equation}
\min_{\tau}\ \ R(\tau) + \lambda\,\Delta_{\rightarrow}(\tau),
\label{eq:rd_main}
\end{equation}
where $\lambda$ controls the compression--fidelity balance under a fixed vocabulary
budget and deployment constraints.


% =========================
% 4. Method
% =========================
\section{Method: Controlled Tokenization}
\label{sec:method}

This section operationalizes the controlled-compression objective
\eqref{eq:rd_lagrangian} into a deployable tokenizer design.
Our goal is to construct a tokenizer $\tau$ that (i) reduces rate $R(\tau)=\mathbb{E}[|Z|]$
and (ii) preserves directed semantic predictability $\Delta_{\rightarrow}(\tau)$ that
drives separability in encoder-only detection.

\subsection{Estimating Directed Distortion with Probes}

To estimate $\Delta_{\rightarrow}(\tau)$, we train a lightweight probe $q\in\mathcal{Q}$
that observes tokenized prefixes $Z_{1:t}$ and predicts $Y_t$.
We use teacher-aligned cross-entropy as a proxy (equivalent to KL up to constants):
\begin{equation}
\widehat{\Delta}_{\rightarrow}(\tau)
=
\min_{q\in\mathcal{Q}}
\ \mathbb{E}\Bigg[\frac{1}{T}\sum_{t=1}^T
\mathbb{E}_{P^\star(\cdot\mid X_{1:t})}\big[-\log q(\cdot\mid Z_{1:t})\big]
\Bigg].
\label{eq:probe_distortion}
\end{equation}

\subsection{Vocabulary Induction via Gain--Distortion Optimization}

We construct the tokenizer by greedy vocabulary induction using a gain--distortion score.
Let $\mathcal{V}$ be the current vocabulary and $c$ a candidate token.
Define $g(c)$ as the expected reduction in token count and $\delta(c)$ as the increase
in probe-based directed distortion when adding $c$.
We select
\begin{equation}
c^\star = \arg\max_{c\in\mathcal{C}\setminus\mathcal{V}}
\Big(g(c)-\lambda\,\delta(c)\Big),
\label{eq:score}
\end{equation}
until reaching the vocabulary budget.

\begin{algorithm}[t]
\caption{Controlled Tokenization Induction (Offline)}
\label{alg:ctok}
\begin{algorithmic}[1]
\REQUIRE Corpus $\mathcal{D}$, teacher $P^\star$, vocab budget $B$, weight $\lambda$
\STATE Initialize base vocabulary $\mathcal{V}\leftarrow$ atomic symbols (bytes/charset) and required separators
\STATE Train a lightweight probe $q$ to predict semantic targets from tokenized prefixes
\STATE Generate candidate set $\mathcal{C}$ from $\mathcal{D}$ (frequency-filtered)
\WHILE{$|\mathcal{V}|<B$}
  \STATE Estimate compression gain $g(c)$ for $c\in \mathcal{C}\setminus\mathcal{V}$
  \STATE Estimate distortion increment $\delta(c)$ by evaluating probe loss change under $\mathcal{V}\cup\{c\}$
  \STATE Select $c^\star\leftarrow \arg\max_c\big(g(c)-\lambda\,\delta(c)\big)$ and add to $\mathcal{V}$
  \STATE Optionally refresh probe $q$ every $M$ steps
\ENDWHILE
\RETURN Vocabulary $\mathcal{V}$
\end{algorithmic}
\end{algorithm}


\subsection{Deterministic Runtime Tokenization}

The learned vocabulary is deployed as a deterministic runtime tokenizer.
We compile $\mathcal{V}$ into a deterministic matcher (e.g., an FST or equivalent automaton)
that maps inputs to token IDs with predictable performance.
Runtime tokenization proceeds in a single left-to-right pass with no backtracking,
ensuring stable behavior across environments.

% =========================
% 3. CTok System: Controlled Tokenization for Detection
% =========================
\section{CTok System: Controlled Tokenization for Detection}
\label{sec:ctok_system}

This section turns the controlled-compression objective
(Equation~\ref{eq:rd_main}) into a deployable tokenizer \emph{system}.
CTok separates \textbf{offline build-time} induction from \textbf{online runtime}
tokenization: all heavy computation (probe training, candidate scoring, vocabulary
selection) happens offline, while runtime tokenization is a deterministic, compiled
left-to-right matcher.

\subsection{Design requirements (why a ``system'')}
\label{sec:ctok_requirements}

Structured detection pipelines impose constraints beyond typical NLP tokenization.
CTok is designed to satisfy three requirements:

\textbf{(R1) Budgeted efficiency.} Reduce expected token length $R(\tau)=\mathbb{E}[|Z|]$
to improve encoder throughput and memory under fixed deployment budgets.

\textbf{(R2) Semantic fidelity for detection.} Preserve detection-relevant predictability
measured by directed semantic distortion $\Delta_{\rightarrow}(\tau)$ on prefixes
(Equation~\ref{eq:dist_directed_main}), rather than requiring reconstruction.

\textbf{(R3) Deterministic deployability.} Provide a reproducible mapping $X\mapsto Z$
with predictable worst-case behavior, suitable for versioning and audit.

\subsection{Estimating directed semantic distortion with lightweight probes}
\label{sec:ctok_probes}

CTok estimates $\Delta_{\rightarrow}(\tau)$ using lightweight probes that predict
teacher semantics from tokenized prefixes.
In practice we use teacher-aligned cross-entropy (equivalent to KL up to constants)
as a stable estimator:
\begin{equation}
\widehat{\Delta}_{\rightarrow}(\tau)
=
\min_{q\in\mathcal{Q}}
\ \mathbb{E}\Bigg[\frac{1}{T}\sum_{t=1}^T
\mathbb{E}_{P^\star(\cdot\mid X_{1:t})}\big[-\log q(\cdot\mid Z_{1:t})\big]
\Bigg],\qquad Z=\tau(X),
\label{eq:ctok_probe_dist}
\end{equation}
where $\mathcal{Q}$ is intentionally low-capacity (linear/MLP) to keep distortion
estimation fast and robust.
We emphasize that probes are \emph{not} the detector; they are an \emph{interface
diagnostic} used to guide vocabulary induction.

\paragraph{Teachers.}
In the main text, we focus on the label teacher (task labels as semantics).
Structure teachers (prefix-aligned tags/channels) and probe-capacity ablations are
reported in Appendix~\ref{app:probe}.

\subsection{Candidate generation with boundary awareness (optional but practical)}
\label{sec:ctok_candidates}

CTok selects tokens from a candidate set $\mathcal{C}$ constructed from the corpus.
A minimal choice is frequency-filtered substrings/spans.
For structured domains, a simple improvement is to restrict candidates to
\emph{boundary-respecting spans} (e.g., tokens do not cross separators such as
\texttt{=}, \texttt{\&}, \texttt{:}, whitespace/newlines, or markup delimiters).
This preserves span locality and reduces boundary sensitivity; detailed rules and
ablation are deferred to Appendix~\ref{app:token_constraints}.

\subsection{Gain--distortion vocabulary induction}
\label{sec:ctok_induction}

CTok constructs a vocabulary under a fixed budget $B$ by greedily maximizing a
gain--distortion score.
Let $\mathcal{V}$ be the current vocabulary and $\tau_{\mathcal{V}}$ be the deterministic
tokenizer induced by $\mathcal{V}$ under a fixed segmentation rule (left-to-right
longest-match with deterministic tie-breaking).
For a candidate token $c$, define:
\[
g(c) \triangleq R(\tau_{\mathcal{V}}) - R(\tau_{\mathcal{V}\cup\{c\}}),
\qquad
\delta(c) \triangleq \widehat{\Delta}_{\rightarrow}(\tau_{\mathcal{V}\cup\{c\}}) -
\widehat{\Delta}_{\rightarrow}(\tau_{\mathcal{V}}).
\]
We select
\begin{equation}
c^\star = \arg\max_{c\in \mathcal{C}\setminus\mathcal{V}}
\Big(g(c)-\lambda\,\delta(c)\Big),
\label{eq:ctok_score}
\end{equation}
until $|\mathcal{V}|=B$.
This is a greedy approximate optimization of the Lagrangian objective
$R(\tau)+\lambda\Delta_{\rightarrow}(\tau)$; a more formal justification via a
submodular surrogate is provided in Appendix~\ref{app:submodular}.

\begin{algorithm}[t]
\caption{CTok build-time induction (offline)}
\label{alg:ctok_main}
\begin{algorithmic}[1]
\REQUIRE Corpus $\mathcal{D}$, teacher $P^\star$, vocab budget $B$, weight $\lambda$
\STATE Initialize base vocabulary $\mathcal{V}\leftarrow$ atomic symbols and required separators
\STATE Train probe(s) $q$ to predict semantic targets from tokenized prefixes
\STATE Build candidate set $\mathcal{C}$ from $\mathcal{D}$ (frequency-filtered; optionally boundary-respecting)
\WHILE{$|\mathcal{V}|<B$}
  \STATE Estimate compression gain $g(c)$ for $c\in \mathcal{C}\setminus\mathcal{V}$
  \STATE Estimate distortion increment $\delta(c)$ using probe loss under $\mathcal{V}\cup\{c\}$
  \STATE Add $c^\star\leftarrow \arg\max_c\big(g(c)-\lambda\,\delta(c)\big)$ to $\mathcal{V}$
  \STATE Optionally refresh probes every $M$ steps
\ENDWHILE
\RETURN Vocabulary $\mathcal{V}$
\end{algorithmic}
\end{algorithm}

\subsection{Deterministic compilation and runtime tokenization}
\label{sec:ctok_runtime}

A tokenizer used in detection is a \emph{system component} on the critical path.
To ensure predictable and reproducible behavior, CTok compiles the learned vocabulary
$\mathcal{V}$ and segmentation rule into a deterministic matcher (FST/DFA-equivalent)
executed in a single left-to-right pass with no backtracking.

\paragraph{Determinism.}
Given a fixed artifact (vocabulary + compiled matcher), the mapping $X\mapsto Z$ is
uniquely defined.
This supports reproducible training/inference, auditing, and stable deployment across
environments.

\paragraph{Worst-case runtime.}
For input length $n$, runtime tokenization runs in $\mathcal{O}(n)$ time with a stable
constant factor determined by the compiled artifact size.
This worst-case guarantee is independent of input distribution and avoids heuristic
backtracking behaviors.

\subsection{Offline vs.\ online cost}
\label{sec:ctok_offline_online}

Build-time induction (probe training and candidate scoring) is performed offline and can
be amortized across deployments.
At runtime, CTok performs only deterministic matching and token emission.
This separation is intentional: CTok spends complexity offline to keep online tokenization
lightweight and predictable.

\subsection{What CTok guarantees (in main text)}
\label{sec:ctok_summary}

CTok provides a compact set of properties needed for a drop-in detection interface:
(i) reduced expected token length (rate),
(ii) controlled directed semantic distortion measured on prefixes,
and (iii) deterministic, linear-time runtime tokenization with versionable artifacts.
Additional engineering details (artifact sizes, microbenchmarks, compilation pipeline)
are reported in Appendix~\ref{app:systems_extra}.


\section{Experiments}
\label{sec:experiments}

We present three main-text experiments that directly validate the paper's central claims.
\textbf{E1} shows CTok improves the rate--distortion frontier under matched vocabulary budgets.
\textbf{E2} shows that under equal training compute, CTok matches or improves detection while
using fewer tokens, improving throughput and memory.
\textbf{E3} shows CTok is a more stable interface under semantics-preserving perturbations,
reducing boundary sensitivity and prediction/embedding drift.
All additional results (more budgets, more model sizes, token-free baselines, extended
diagnostics, and ablations) are deferred to the appendix.


\subsection{Setup}
\label{sec:exp_setup}

We evaluate three structured domains (Table~\ref{tab:datasets}) and compare BPE, byte-level,
boundary-healing, and CTok under matched vocabulary budgets (main text uses \texttt{TODO}).
All results fine-tune the same encoder-only backbone; unless stated otherwise we control compute
by fixing the total number of training tokens processed (Appendix~\ref{app:protocols}).
Additional tokenizers (e.g., Unigram), model-size scaling, token-free baselines, and ablations are
reported in Appendix~\ref{app:extra_results}.


\subsection{E1: CTok improves the rate--distortion frontier}
\label{sec:exp_e1}

\paragraph{Question.}
Does CTok reduce token rate $R(\tau)$ \emph{without} increasing directed semantic
distortion $\Delta_{\rightarrow}(\tau)$, compared to frequency-driven subword methods?

\paragraph{Method.}
For each tokenizer and vocab budget, we measure mean/P95 token length (rate) and
estimate directed distortion using probe cross-entropy on tokenized prefixes
(Section~\ref{sec:method}). We plot $R(\tau)$ vs.\ $\widehat{\Delta}_{\rightarrow}(\tau)$.

\subsection{E1: CTok improves the rate--distortion frontier}
\label{sec:exp_e1}

\paragraph{What this tests.}
E1 validates the core objective: can we reduce token rate while controlling directed
semantic distortion, compared to frequency-driven subword tokenizers?

\paragraph{Result.}
Figure~\ref{fig:rd_frontier} shows that CTok consistently improves the frontier under
matched vocabulary budgets: for a comparable distortion level, CTok yields shorter token
sequences; conversely, at comparable rate, CTok yields lower directed distortion.
This supports the central claim that distortion-controlled induction preserves
detection-relevant predictability at the tokenization interface rather than merely
compressing frequent substrings.

% ---- Table 1: Dataset summary (main text) ----
\begin{table}[t]
\centering
\small
\caption{Datasets and tasks (main text). We report dataset sizes and basic input statistics.
See Appendix~\ref{app:exp_details} for splits, normalization, and additional domains/settings.}
\label{tab:datasets}
\begin{tabular}{lcccc}
\toprule
Domain & Task & \#Train/\#Val/\#Test & Avg bytes & Notes \\
\midrule
HTTP/WAF & attack detection & \texttt{TODO}/\texttt{TODO}/\texttt{TODO} & \texttt{TODO} &
delimiters: \texttt{\& = : ?} \\
Phishing HTML & binary cls & \texttt{TODO}/\texttt{TODO}/\texttt{TODO} & \texttt{TODO} &
markup + URLs \\
HDFS logs & anomaly detection & \texttt{TODO}/\texttt{TODO}/\texttt{TODO} & \texttt{TODO} &
sessionized sequences \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Result.}
Figure~\ref{fig:rd_frontier} shows that CTok consistently attains lower rate at the same
(or lower) directed distortion, indicating a better rate--distortion trade-off for
detection-centric semantics.

% ---- Fig 1: Rate--Distortion frontier (main text) ----
\begin{figure}[t]
\centering
% \includegraphics[width=\linewidth]{figs/rd_frontier.pdf}
\caption{\textbf{E1: Rate--distortion frontier under matched vocabulary budgets.}
Each point is a tokenizer (BPE/Unigram/byte-level/boundary-healing/CTok) at a fixed
vocabulary size (main text: \texttt{TODO}, others in Appendix~\ref{app:budgets}).
$x$-axis: token rate $R(\tau)=\mathbb{E}[|Z|]$; $y$-axis: estimated directed semantic
distortion $\widehat{\Delta}_{\rightarrow}(\tau)$ (probe cross-entropy on prefixes).
CTok achieves lower rate at the same (or lower) distortion across domains, indicating
a better controlled-compression trade-off for detection.}
\label{fig:rd_frontier}
\end{figure}


\subsection{E2: Under equal compute, CTok matches or improves detection while using fewer tokens}
\label{sec:exp_e2}

\paragraph{What this tests.}
E2 tests whether the improved rate--distortion properties translate into end-task
detection benefits under a fixed compute envelope, rather than trading accuracy for compression.

\paragraph{Result.}
Table~\ref{tab:main_results} shows that CTok reduces token length (mean and tail) and
improves system efficiency (tokens/s and peak memory) while matching or improving the
task metric under the same total training-token budget.
This indicates that CTok's compression preserves detection-relevant semantics as measured
by directed distortion, enabling better deployability without changing the encoder.
Additional budgets and model-size scaling are reported in Appendix~\ref{app:extra_results}.


% ---- Table 2: Main results (accuracy + efficiency) ----
\begin{table*}[t]
\centering
\small
\caption{\textbf{E2: Equal-compute detection performance and efficiency.}
All tokenizers fine-tune the same encoder-only backbone under a fixed total training-token budget
(\texttt{TODO} tokens; protocol in Appendix~\ref{app:protocols}). We report end-task metrics together
with interface-level length and system efficiency. CTok matches or improves detection while reducing
token length and improving throughput/memory.}
\label{tab:main_results}
\begin{tabular}{llcccccc}
\toprule
Domain & Tokenizer & Metric (AUROC/F1/Acc) & Avg len & P95 len & tokens/s & peak mem & notes \\
\midrule
HTTP/WAF & BPE & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \\
HTTP/WAF & CTok & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \\
\midrule
HTML & BPE & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \\
HTML & CTok & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \\
\midrule
HDFS & BPE & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \\
HDFS & CTok & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \texttt{TODO} & \\
\bottomrule
\end{tabular}
\end{table*}

% ---- Fig 2: Robustness / boundary sensitivity (main text) ----
\begin{figure}[t]
\centering
% \includegraphics[width=\linewidth]{figs/robustness.pdf}
\caption{\textbf{E3: Interface robustness under semantics-preserving perturbations.}
We apply benign format-drift transformations (whitespace/delimiter normalization, attribute reordering,
header casing/order where applicable) and measure: (i) tokenization instability (token edit distance or
length jitter), (ii) prediction stability (flip rate / confidence drift), and (iii) representation
stability (embedding drift). CTok substantially reduces boundary sensitivity across domains.
Extended perturbation sets and per-domain breakdowns are in Appendix~\ref{app:robustness}.}
\label{fig:robustness}
\end{figure}

\subsection{E3: CTok provides a more stable interface under benign format drift}
\label{sec:exp_e3}

\paragraph{What this tests.}
E3 evaluates tokenization as a \emph{system interface}: small, semantics-preserving edits should not
cause large segmentation changes or unstable predictions/embeddings in detection pipelines.

\paragraph{Result.}
Figure~\ref{fig:robustness} shows CTok consistently reduces boundary sensitivity compared to subword
and heuristic baselines, improving tokenization stability and reducing prediction/embedding drift under
benign perturbations. This supports the mechanism implied by our objective: controlling directed semantic
distortion preserves prefix-based predictability, yielding a more stable interface for encoder-only
detection. Correlations between distortion and robustness (and extended diagnostics) are reported in
Appendix~\ref{app:diagnostics}.

\section{Related Work}
\label{sec:related}

\paragraph{NLP-centric subword tokenization and engineering.}
Subword tokenizers such as BPE and Unigram are the default interface in NLP, balancing
vocabulary size and sequence length while retaining broad coverage
\citep{sennrich2016subword,kudo2018sentencepiece}.
Encoder-only models (e.g., BERT-style) rely on WordPiece-like segmentation as a
practical front-end \citep{devlin2019bert}, and prior work has studied fast
implementations and system trade-offs for large-scale usage \citep{song2021fastwordpiece}.
Our setting differs in both \emph{data regime} and \emph{objective}: we target structured
and semi-structured detection streams where semantics are often boundary- and
structure-driven, and we design tokenization explicitly as \emph{controlled compression}
with a task-aligned distortion rather than frequency-driven segmentation.

\paragraph{Token-free, byte/character-level, and below-byte alternatives.}
Tokenizer-free and byte/character-level models improve robustness to segmentation
brittleness by operating on raw units, but typically incur much longer sequences and
higher compute, motivating architectural changes to remain efficient at scale
\citep{clark2022canine,xue2022byt5}.
In budgeted detection pipelines, practitioners often face strong \emph{drop-in}
constraints: stable encoder-only backbones, reproducible preprocessing, and limited
headroom for architectural re-design.
CTok targets this regime by keeping the modeling interface fixed and treating tokenization
as the primary lever for efficiency and semantic predictability, while still providing a
meaningful comparison to tokenizer-free baselines in the appendix.

\paragraph{Tokenization as compression with semantic fidelity objectives.}
Recent perspectives argue that tokenization should be studied through semantic objectives
rather than exact reconstruction, and that directed formulations are natural in sequential
systems \citep{bai2025token}.
CTok adopts this viewpoint for detection: we formalize a \emph{directed semantic distortion}
via teacher--student KL on prefixes and optimize a rate--distortion trade-off tailored to
structure-driven semantics.
Compared to general semantic tokenization objectives, our emphasis is on (i) deployable
tokenizer induction under vocabulary budgets and (ii) detection-centric evaluation under
equal compute and interface robustness tests.

\paragraph{Finite-state views and deployable tokenizers.}
Tokenization is a system component: it must be fast, deterministic, and reproducible.
Finite-state perspectives show that segmentation and normalization pipelines can be
represented as finite-state transduction, enabling analyzable behavior and deterministic
execution \citep{cognetta2024fst}.
CTok leverages this systems viewpoint by compiling learned tokenization rules into a
deterministic matcher (FST/DFA-equivalent) and treating the resulting artifacts as
versionable components.
This is complementary to WFST tooling traditions such as OpenFST \citep{allauzen2007openfst}.

\paragraph{Structured detection and log/security sequence modeling.}
A substantial literature applies sequence models to logs and event streams for anomaly
detection and classification (e.g., DeepLog, LogAnomaly, LogBERT)
\citep{du2017deeplog,meng2019loganomaly,guo2021logbert}.
These works validate the utility of encoder-style representations for detection, but
tokenization is often inherited from generic NLP practice or treated as an implementation
detail.
Our work instead elevates tokenization to the design object and provides an objective and
system to control interface compression and semantic predictability, with evaluation that
includes both end-task outcomes and interface-level robustness.

\paragraph{Diagnostics: dependency and representation geometry.}
We use Granger-style predictability as a diagnostic for dependency disruption
\citep{granger1969causality} and complement it with embedding-space diagnostics such as
neighborhood preservation and clusterability proxies \citep{venna2001neighborhood,rousseeuw1987silhouettes}.
In our framework, these are explanatory tools (extended in the appendix) that help connect
directed semantic distortion to detection-relevant stability and separability.

\section{Conclusion}
\label{sec:conclusion}

We studied tokenization as a first-class \emph{system interface} for encoder-only
detection on structured and semi-structured inputs.
We formulated tokenization as \emph{controlled compression}: reduce token rate while
controlling a task-aligned \emph{directed semantic distortion} defined via teacher--student
KL on prefixes, rather than requiring reconstruction.
We introduced CTok, an offline-induced and deterministically compiled tokenizer system,
and showed that it improves the rate--distortion frontier, yields better efficiency under
equal compute, and provides a more stable interface under semantics-preserving format drift.
Together, these results suggest that properly optimizing the tokenization interface can
simultaneously improve deployability and detection robustness without changing the encoder
architecture.

\section*{Impact Statement}

This paper studies tokenization as a system interface for structured detection workloads.
Improving efficiency and semantic predictability can enable more deployable security and
monitoring tools, potentially reducing the cost of protecting systems and users.
As with any detection technology, misuse is possible (e.g., privacy-invasive monitoring);
responsible governance, transparency, and privacy-preserving practices are important.
Our method focuses on representational efficiency and does not introduce new capabilities
for intrusion or evasion; we encourage responsible evaluation and deployment aligned with
applicable policies and regulations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\appendix

\section{Experimental Details and Protocols}
\label{app:exp_details}

This appendix provides the full experimental setup needed to reproduce the main-text
results: datasets/splits, normalization, tokenizers, models, compute control, training
hyperparameters, and metrics. Unless otherwise stated, all results are averaged over
multiple seeds; report mean and standard deviation.

\subsection{Datasets, tasks, and splits}
\label{app:datasets_splits}

\paragraph{HTTP/WAF requests.}
\textbf{Task:} binary attack detection (optionally multi-class attack type).
\textbf{Source:} \texttt{TODO: dataset name/version}.
\textbf{Splits:} \texttt{TODO: train/val/test sizes}.
\textbf{Notes:} preserve request structure (method/path/query/headers/body), retain
delimiters (\texttt{? \& = : /} etc.).

\paragraph{Phishing HTML.}
\textbf{Task:} phishing vs.\ benign classification.
\textbf{Source:} \texttt{TODO: dataset name/version}.
\textbf{Splits:} \texttt{TODO}.
\textbf{Notes:} raw HTML string, includes tags/attributes and embedded URLs.

\paragraph{HDFS logs.}
\textbf{Task:} session anomaly detection (normal vs.\ abnormal).
\textbf{Source:} HDFS\_v1 from LogHub (\texttt{TODO: cite or version}).
\textbf{Splits:} \texttt{TODO}.
\textbf{Notes:} sessionization rule (\texttt{TODO}), log lines concatenated with explicit
line separators.

\subsection{Normalization and preprocessing}
\label{app:preprocess}

We apply minimal, structure-preserving normalization:
\begin{itemize}
  \item Unicode normalization (\texttt{TODO: NFC/NFKC or none})
  \item Newline normalization (\texttt{CRLF} $\rightarrow$ \texttt{LF})
  \item Optional whitespace normalization (\texttt{TODO: exactly what})
\end{itemize}
We avoid semantic rewriting (e.g., URL decoding or header canonicalization) unless
explicitly stated for a robustness transformation family (Appendix~\ref{app:robustness}).

\subsection{Tokenizers and vocabulary budgets}
\label{app:protocols_tokenizers}

\paragraph{Baselines.}
We evaluate:
(i) BPE, (ii) Unigram (SentencePiece-style), (iii) byte-level tokenization,
(iv) boundary-healing heuristic, and (v) CTok (ours).

\paragraph{Vocabulary budgets.}
Main text uses \texttt{TODO: e.g., 16k}. Additional budgets (e.g., 8k/32k) are reported
in Appendix~\ref{app:budgets} and Appendix~\ref{app:extra_results}.

\paragraph{Segmentation rules.}
For CTok, we use a deterministic left-to-right segmentation (longest-match with
deterministic tie-breaking).
For baselines, specify the exact implementation/library and its deterministic settings:
\texttt{TODO: library versions, settings, seed}.

\subsection{Models and fine-tuning}
\label{app:models}

\paragraph{Encoder-only backbone.}
Main text: \texttt{TODO: model family and size (e.g., RoBERTa-base class)}.
We adapt only the embedding/vocabulary interface to each tokenizer; the architecture,
optimizer, and training schedule are unchanged.

\paragraph{Scaling.}
Additional model sizes (Small/Base/Large) are reported in Appendix~\ref{app:scaling}.

\subsection{Compute control protocols}
\label{app:protocols}

We report two regimes.

\paragraph{Equal-compute (preferred).}
We fix the total number of \emph{training tokens processed}:
\[
\texttt{total\_train\_tokens} = \texttt{TODO}.
\]
For each tokenizer, we adjust effective sequence packing and/or number of steps so that
the total number of tokens seen by the model is matched.
This isolates performance changes due to tokenization rather than differing compute.

\paragraph{Equal-model (system efficiency).}
We fix model size, max sequence length, batch size, optimizer, and number of steps.
We compare induced differences in (i) token length, (ii) throughput (tokens/s, samples/s),
and (iii) peak memory.

\paragraph{Wall-clock control (optional).}
If reporting wall-clock matched runs, provide:
hardware, framework, mixed precision, and dataloader settings (Appendix~\ref{app:hardware}).

\subsection{Training hyperparameters and seeds}
\label{app:hyperparams}

Provide a single source of truth table (per domain if needed):
\begin{itemize}
  \item optimizer: AdamW (\texttt{TODO: betas, eps, weight decay})
  \item LR schedule: \texttt{TODO: linear warmup, cosine, etc.}
  \item warmup steps/ratio: \texttt{TODO}
  \item batch size: \texttt{TODO} (global and per GPU)
  \item max length: \texttt{TODO} (tokens)
  \item dropout: \texttt{TODO}
  \item epochs/steps: \texttt{TODO} (and how derived under equal-compute)
  \item early stopping: \texttt{TODO: criterion, patience}
  \item seeds: \texttt{TODO: list or count}
\end{itemize}

\subsection{Metrics}
\label{app:metrics}

\paragraph{Task metrics.}
Report AUROC/F1/accuracy as appropriate per dataset (\texttt{TODO: specify}).

\paragraph{Rate and length statistics.}
Mean/P95/P99 token lengths, token-to-byte ratio, and length jitter under perturbations.

\paragraph{System metrics.}
Training throughput (tokens/s, samples/s), step time, peak GPU memory.
Inference latency/throughput (\texttt{TODO: batch sizes}).
Tokenizer microbenchmarks are in Appendix~\ref{app:systems_extra}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix: Additional Main Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Additional Main Results}
\label{app:extra_results}

This appendix extends the main-text experiments with additional settings:
vocabulary budgets, model sizes, alternative baselines, and per-domain breakdowns.

\subsection{Additional vocabulary budgets}
\label{app:budgets}

Report E1/E2/E3 for \texttt{TODO: 8k/32k} budgets. Include:
(i) rate--distortion plots, (ii) equal-compute performance tables, and (iii) robustness.

\subsection{Model-size scaling}
\label{app:scaling}

Report results for Small/Base/Large backbones. Emphasize whether CTok benefits persist
under scaling and whether efficiency gains translate similarly.

\subsection{Expanded tokenizer set}
\label{app:more_tokenizers}

Add Unigram (and any other variants) to the comparisons.
If including multiple BPE variants, describe and justify each.

\subsection{Wall-clock matched results (optional)}
\label{app:wallclock}

If you include wall-clock matching, provide a table of (i) time-to-train, (ii) end
metric, (iii) throughput, and (iv) stability indicators.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix: Token-free and Alternative Baselines
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Token-free and Alternative Baselines}
\label{app:tokenfree}

This appendix reports non-drop-in alternatives and classic baselines.

\subsection{Tokenizer-free/character-level encoders}
\label{app:tokenfree_models}

Include CANINE/ByT5/char-level variants as strong robustness-oriented baselines.
Report compute-aligned comparisons (total training tokens, wall-clock, or FLOPs) and
clarify that these baselines change the modeling interface.

\subsection{Classic non-neural baselines}
\label{app:classic_baselines}

Report character $n$-gram + logistic regression / linear SVM.
Provide feature settings ($n$ range, hashing, regularization) and performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix: Ablations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ablations of CTok Design Choices}
\label{app:ablations}

\subsection{Compression-only induction ($\lambda=0$)}
\label{app:ablate_lambda0}

Compare CTok with $\lambda=0$ vs.\ tuned $\lambda$.
Report rate--distortion and end-task outcomes; highlight failure modes if distortion is
not controlled.

\subsection{Probe capacity and features}
\label{app:ablate_probe_capacity}

Compare linear vs.\ MLP probes, different context windows, and alternative prefix features
(\texttt{TODO: specify}).
Report estimator stability and whether induction outcomes change.

\subsection{Teacher choice: label vs.\ structure teacher}
\label{app:ablate_teacher}

Define structure teachers (field/channel tags) and evaluate whether they improve
robustness and downstream performance beyond label-only.

\subsection{Candidate constraints: boundary-aware vs.\ frequency-only}
\label{app:token_constraints}

Evaluate boundary-respecting candidates and span-local constraints.
Report robustness impacts and rate--distortion changes.

\subsection{Runtime variants: compiled deterministic vs.\ heuristic}
\label{app:ablate_runtime}

Compare deterministic compiled matcher vs.\ heuristic implementations.
Report tokenization latency and determinism checks (Appendix~\ref{app:systems_extra}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix: Robustness
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Robustness: Perturbation Sets and Extended Results}
\label{app:robustness}

This appendix defines the semantics-preserving perturbation families and reports
extended robustness results.

\subsection{Perturbation families}
\label{app:perturbations}

\paragraph{HTTP/WAF.}
Whitespace normalization, delimiter normalization, header casing, header ordering within
order-invariant sets, query-parameter reordering where semantics are invariant
(\texttt{TODO: define invariance assumptions}).

\paragraph{HTML.}
Whitespace/line-break normalization, attribute reordering within a tag, benign formatting
changes that preserve DOM semantics (\texttt{TODO: define}).

\paragraph{HDFS logs.}
Spacing normalization, benign field formatting changes (e.g., number padding), delimiter
variants (\texttt{TODO}).

\subsection{Robustness metrics}
\label{app:robustness_metrics}

Report:
(i) token edit distance and normalized length jitter,
(ii) prediction flip rate and confidence drift,
(iii) embedding drift $\|e(X)-e(\tilde X)\|_2$ and neighbor changes.

\subsection{Extended robustness results}
\label{app:robustness_results}

Provide per-domain breakdown figures/tables and per-perturbation-category analyses.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix: Diagnostics
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Diagnostics: Dependency and Representation Geometry}
\label{app:diagnostics}

This appendix provides the extended diagnostics used to interpret CTok's effects beyond
end-task performance and robustness.

\subsection{Directed-information viewpoint of prefix distortion}
\label{app:directed-info}

This section clarifies how the directed semantic distortion relates to information flow
in sequential settings.

\paragraph{Conditional directed information.}
Let $X_{1:T}$ be a raw symbolic sequence, and let $Y_{1:T}$ be semantic targets aligned
with time (e.g., labels per position, field tags, or event types).
A standard notion of causal information flow is (conditional) directed information,
defined as
\begin{equation}
I^\star(X_{1:T}\rightarrow Y_{1:T}\mid W_{1:T})
\triangleq
\sum_{t=1}^T I^\star(X_{1:t};Y_t\mid Y_{1:t-1},W_{1:t}),
\tag{G.1}\label{eq:app_dirinfo_def}
\end{equation}
where $W_{1:T}$ is side information (here we will take $W_{1:T}=Z_{1:T}$ or other
observables).

Our directed distortion (Lemma~\ref{lem:directed-identity} in Appendix~\ref{app:theory}) is
\[
\Delta_{\rightarrow}(\tau)
=
\frac{1}{T}\sum_{t=1}^T
I^\star(Y_t;X_{1:t}\mid Z_{1:t}).
\]
This is not identical to directed information in \eqref{eq:app_dirinfo_def} because it
does not condition on past semantics $Y_{1:t-1}$.
Nevertheless, it upper-bounds a directed-information quantity and matches it under mild
assumptions.

\begin{proposition}[Upper bound on conditional directed information]
\label{prop:dirinfo_upper}
For any processes $(X_{1:T},Y_{1:T})$ and any tokenization $Z=\tau(X)$,
\begin{equation}
\sum_{t=1}^T I^\star(X_{1:t};Y_t\mid Y_{1:t-1},Z_{1:t})
\ \le\
\sum_{t=1}^T I^\star(X_{1:t};Y_t\mid Z_{1:t})
= T\,\Delta_{\rightarrow}(\tau).
\tag{G.2}\label{eq:app_dirinfo_upper}
\end{equation}
\end{proposition}

\begin{proof}
For each $t$, conditioning reduces mutual information:
$I(A;B\mid C,D)\le I(A;B\mid C)$.
Apply with $A=X_{1:t}$, $B=Y_t$, $C=Z_{1:t}$, $D=Y_{1:t-1}$, and sum over $t$.
\end{proof}

\subsection{Granger-style predictability as conditional mutual information}
\label{app:granger-cmi}

The main text defines a Granger-style influence score by comparing predictive losses.
Here we show that under log-loss this difference is exactly a conditional mutual
information (CMI).

\paragraph{Setup.}
Let $\{U_t\}_{t\ge 1}$ be a multivariate process with channels
$U_t^{(1)},\dots,U_t^{(m)}$.
Fix two channels $i\neq j$.
Let $\mathcal{L}(A\mid B)$ denote the optimal expected log-loss for predicting $A$ given
$B$:
\begin{equation}
\mathcal{L}(A\mid B)
\triangleq
\min_{q(\cdot\mid B)} \ \mathbb{E}\big[-\log q(A\mid B)\big].
\tag{H.1}\label{eq:app_logloss_def}
\end{equation}

\begin{lemma}[Log-loss Granger difference equals CMI]
\label{lem:granger_cmi}
Let $A=U_t^{(j)}$, $B=U_{1:t-1}^{(\neg i)}$, and $C=U_{1:t-1}^{(i)}$.
Then
\begin{equation}
\mathcal{L}(A\mid B) - \mathcal{L}(A\mid B,C)
=
I^\star(A;C\mid B).
\tag{H.2}\label{eq:app_granger_cmi}
\end{equation}
\end{lemma}

\begin{proof}
By the same variational argument as Lemma~\ref{lem:dist-identity}, the optimal log-loss
is the conditional entropy:
$\mathcal{L}(A\mid B)=H^\star(A\mid B)$ and
$\mathcal{L}(A\mid B,C)=H^\star(A\mid B,C)$.
Therefore
\[
\mathcal{L}(A\mid B)-\mathcal{L}(A\mid B,C)
=
H^\star(A\mid B)-H^\star(A\mid B,C)
=
I^\star(A;C\mid B).
\]
\end{proof}

\subsection{From distortion to representation geometry}
\label{app:info-geometry}

This section formalizes a chain from semantic distortion at the tokenizer interface to
limits on downstream representations, and illustrates separability effects in a stylized
Gaussian discriminant model.

\subsubsection{Information chain: interface loss bounds downstream information}

Let $Z=\tau(X)$ and let $e=\phi(Z)$ be any deterministic representation (e.g., the pooled
encoder embedding used for detection).
Then by data processing,
\begin{equation}
I^\star(Y;e) \le I^\star(Y;Z).
\tag{I.1}\label{eq:app_dp1}
\end{equation}
Moreover, because $Z$ is a function of $X$,
\begin{equation}
I^\star(Y;Z) = I^\star(Y;X) - I^\star(Y;X\mid Z) = I^\star(Y;X) - \Delta(\tau),
\tag{I.2}\label{eq:app_info_gap}
\end{equation}
where the last equality uses Lemma~\ref{lem:dist-identity}.
Combining \eqref{eq:app_dp1}--\eqref{eq:app_info_gap},
\begin{equation}
I^\star(Y;e)
\ \le\
I^\star(Y;X) - \Delta(\tau).
\tag{I.3}\label{eq:app_info_chain}
\end{equation}

\subsubsection{Gaussian discriminant illustration: distortion behaves like added noise}

Assume binary classes $Y\in\{-1,+1\}$ with equal priors.
Let $S\in\mathbb{R}^d$ be an ideal detection-relevant feature with
\begin{equation}
S\mid Y=y \sim \mathcal{N}(y\mu,\, \Sigma),
\tag{I.4}\label{eq:app_gauss}
\end{equation}
and model tokenization-induced semantic loss as additive interface noise:
\begin{equation}
\tilde S = S + \varepsilon,\qquad \varepsilon\sim \mathcal{N}(0,\Sigma_\tau),
\tag{I.5}\label{eq:app_noise}
\end{equation}
independent of $(S,Y)$.
Then $\tilde S\mid Y=y\sim \mathcal{N}(y\mu, \Sigma+\Sigma_\tau)$.
A separability proxy aligned with LDA is the squared Mahalanobis distance
\begin{equation}
\mathcal{D}_\tau^2 \triangleq \mu^\top(\Sigma+\Sigma_\tau)^{-1}\mu.
\tag{I.6}\label{eq:app_mahal}
\end{equation}

\begin{proposition}[Added interface noise reduces separability]
\label{prop:noise_sep}
If $\Sigma_\tau^{(1)} \preceq \Sigma_\tau^{(2)}$ (PSD order), then
$\mathcal{D}_{\tau^{(1)}}^2 \ge \mathcal{D}_{\tau^{(2)}}^2$.
\end{proposition}

\begin{proof}
If $A\preceq B$ and both are PSD, then $(\Sigma+A)^{-1}\succeq(\Sigma+B)^{-1}$ for
$\Sigma\succ 0$ (order-reversing property of matrix inverse).
Premultiplying and postmultiplying by $\mu$ yields the result.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix: System Implementation and Microbenchmarks
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{System Implementation and Microbenchmarks}
\label{app:systems_extra}

This appendix documents the CTok build-time and runtime implementation, compilation,
artifact size, determinism tests, and tokenizer microbenchmarks.

\subsection{Compilation pipeline (FST/DFA-equivalent)}
\label{app:fst_pipeline}

Describe the compilation pipeline:
(i) vocabulary + segmentation rule, (ii) automaton construction, (iii) determinization,
(iv) minimization/optimization, (v) emission of runtime artifact.
Specify tooling/library versions (\texttt{TODO}).

\subsection{Determinism and golden tests}
\label{app:golden}

Define golden tokenization tests used to verify $X\mapsto Z$ stability across platforms.
Report whether artifacts are byte-identical and whether outputs match exactly.

\subsection{Tokenizer microbenchmarks}
\label{app:microbench}

Report CPU throughput (MB/s, samples/s), P50/P95 latency, and memory usage for each
tokenizer implementation. Provide the benchmark harness and settings.

\subsection{End-to-end efficiency}
\label{app:e2e_eff}

Report training throughput/peak memory and inference latency/throughput breakdowns.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix: Additional Theory and Proofs (COMPLETE, from your current content)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Additional Theory and Proofs}
\label{app:theory}

\subsection{Setup and notation}
\label{app:setup}

Let $(X,Y)$ denote a pair of raw symbolic inputs and semantic targets.
In our paper, semantics can be the detection label $Y$ (label teacher) or structured
targets $Y_{1:T}$ aligned to prefixes (structure teacher).
We write $Z=\tau(X)$ for the token sequence produced by a (deterministic) tokenizer
$\tau:\Sigma^\ast\rightarrow\mathcal{V}^\ast$.

To keep the definition general, we introduce a \emph{teacher} conditional distribution
$P^\star(Y\mid X)$ capturing the semantics available from raw inputs.
We assume a joint distribution $P^\star(X,Y)$ with conditional $P^\star(Y\mid X)$.
The student predictor observes only tokenized inputs and belongs to a class
$\mathcal{Q}$ of conditional distributions $q(Y\mid Z)$.
All expectations are taken with respect to $P^\star(X,Y)$ and the induced $Z=\tau(X)$.

\subsection{Variational form: distortion equals a conditional-entropy gap}
\label{app:dist-identity}

We restate the (global) semantic distortion:
\begin{equation}
\Delta(\tau)
\triangleq
\min_{q\in\mathcal{Q}}
\ \mathbb{E}\!\left[\mathrm{KL}\!\left(P^\star(Y\mid X)\ \Vert\ q(Y\mid Z)\right)\right],
\quad Z=\tau(X).
\tag{A.1}\label{eq:app_dist_global}
\end{equation}

\paragraph{Unrestricted predictor class.}
When $\mathcal{Q}$ contains all conditional distributions over $(Y\mid Z)$, the
minimizer is the teacher-consistent posterior $q^\star(Y\mid Z)=P^\star(Y\mid Z)$.
This yields an information-theoretic identity.

\begin{lemma}[Distortion identity]
\label{lem:dist-identity}
Assume $\mathcal{Q}$ contains all conditionals $q(\cdot\mid Z)$.
Then
\begin{equation}
\Delta(\tau)
=
\mathbb{E}\!\left[\mathrm{KL}\!\left(P^\star(Y\mid X)\ \Vert\ P^\star(Y\mid Z)\right)\right]
=
H^\star(Y\mid Z) - H^\star(Y\mid X),
\tag{A.2}\label{eq:app_entropy_gap}
\end{equation}
where $H^\star(\cdot\mid\cdot)$ denotes conditional entropy under $P^\star$.
Moreover, since $Z=\tau(X)$ is a deterministic function of $X$,
\begin{equation}
\Delta(\tau) = I^\star(Y;X\mid Z).
\tag{A.3}\label{eq:app_cmi}
\end{equation}
\end{lemma}

\begin{proof}
Let $q$ be any conditional distribution $q(Y\mid Z)$.
Taking expectation over $P^\star(X,Y)$, we have
\[
\mathbb{E}\left[\mathrm{KL}(P^\star(Y\mid X)\Vert q(Y\mid Z))\right]
=
\mathbb{E}\left[\log P^\star(Y\mid X) - \log q(Y\mid Z)\right].
\]
The first term is $-\!H^\star(Y\mid X)$ (a constant w.r.t.\ $q$).
The second term is the cross-entropy between $P^\star(Y\mid Z)$ and $q(Y\mid Z)$:
\[
\mathbb{E}\left[-\log q(Y\mid Z)\right]
=
H^\star(Y\mid Z) + \mathbb{E}\left[\mathrm{KL}(P^\star(Y\mid Z)\Vert q(Y\mid Z))\right].
\]
Thus the objective equals
\[
\left(H^\star(Y\mid Z)-H^\star(Y\mid X)\right)
+
\mathbb{E}\left[\mathrm{KL}(P^\star(Y\mid Z)\Vert q(Y\mid Z))\right],
\]
which is minimized when $q(Y\mid Z)=P^\star(Y\mid Z)$, giving \eqref{eq:app_entropy_gap}.
For \eqref{eq:app_cmi}, note that because $Z$ is deterministic from $X$,
$H^\star(Y\mid X,Z)=H^\star(Y\mid X)$. Hence
\[
I^\star(Y;X\mid Z)=H^\star(Y\mid Z)-H^\star(Y\mid X,Z)=H^\star(Y\mid Z)-H^\star(Y\mid X).
\]
\end{proof}

\paragraph{Interpretation.}
Lemma~\ref{lem:dist-identity} shows distortion measures the \emph{irrecoverable semantic
uncertainty introduced by the tokenization interface}: it is the conditional mutual
information between semantics $Y$ and raw input $X$ given tokens $Z$.
This is a \emph{task-aligned} distortion: it does not demand reconstructing $X$ and is
zero whenever $Z$ preserves all information about $Y$ that is present in $X$.

\subsection{Basic properties: nonnegativity, invariance, and monotonicity}
\label{app:dist-properties}

\begin{proposition}[Nonnegativity and zero-distortion cases]
\label{prop:nonneg}
Under the conditions of Lemma~\ref{lem:dist-identity}, $\Delta(\tau)\ge 0$ with equality
iff $P^\star(Y\mid X)=P^\star(Y\mid Z)$ almost surely. In particular:
\begin{enumerate}
\item If $\tau$ is invertible (i.e., $X$ can be recovered from $Z$), then
$P^\star(Y\mid Z)=P^\star(Y\mid X)$ and $\Delta(\tau)=0$.
\item More generally, $\Delta(\tau)=0$ whenever $Z$ is a sufficient statistic of $X$
for predicting $Y$ under $P^\star$.
\end{enumerate}
\end{proposition}

\begin{proof}
Nonnegativity follows from the nonnegativity of KL divergence in \eqref{eq:app_dist_global}
or from $I^\star(Y;X\mid Z)\ge 0$ in \eqref{eq:app_cmi}.
Equality conditions follow from the characterization of when KL (or conditional mutual
information) is zero.
\end{proof}

\begin{proposition}[Monotonicity under refinement/coarsening]
\label{prop:monotone}
Let $\tau_1,\tau_2$ be tokenizers producing $Z_1=\tau_1(X)$ and $Z_2=\tau_2(X)$.
If $Z_2$ is a measurable function of $Z_1$ (i.e., $\exists \phi$ such that
$Z_2=\phi(Z_1)$), then
\begin{equation}
\Delta(\tau_2) \ge \Delta(\tau_1).
\tag{A.4}\label{eq:app_monotone}
\end{equation}
That is, further coarsening tokens cannot reduce semantic distortion.
\end{proposition}

\begin{proof}
Since $Z_2=\phi(Z_1)$, we have the Markov chain $Y - X - Z_1 - Z_2$ under $P^\star$.
By the data-processing inequality for conditional mutual information,
$I^\star(Y;X\mid Z_2)\ge I^\star(Y;X\mid Z_1)$, and the claim follows from
\eqref{eq:app_cmi}.
\end{proof}

\subsection{Directed semantic distortion over prefixes}
\label{app:directed}

For online/streaming structured data, we consider semantic targets $Y_{1:T}$ aligned
with prefixes $X_{1:t}$ and token prefixes $Z_{1:t}=\tau(X)_{1:t}$.
We restate directed distortion:
\begin{equation}
\Delta_{\rightarrow}(\tau)
\triangleq
\min_{q\in\mathcal{Q}}
\ \mathbb{E}\Bigg[
\frac{1}{T}\sum_{t=1}^T
\mathrm{KL}\!\left(P^\star(Y_t\mid X_{1:t})\ \Vert\ q(Y_t\mid Z_{1:t})\right)
\Bigg].
\tag{B.1}\label{eq:app_dist_directed}
\end{equation}

\begin{lemma}[Directed distortion identity]
\label{lem:directed-identity}
Assume $\mathcal{Q}$ contains all conditionals $q(\cdot\mid Z_{1:t})$ for each $t$.
Then
\begin{equation}
\Delta_{\rightarrow}(\tau)
=
\frac{1}{T}\sum_{t=1}^T
\left(H^\star(Y_t\mid Z_{1:t}) - H^\star(Y_t\mid X_{1:t})\right)
=
\frac{1}{T}\sum_{t=1}^T I^\star(Y_t; X_{1:t}\mid Z_{1:t}).
\tag{B.2}\label{eq:app_directed_entropy_gap}
\end{equation}
\end{lemma}

\begin{proof}
Apply Lemma~\ref{lem:dist-identity} pointwise to each time $t$, noting that
$Z_{1:t}$ is a deterministic function of $X_{1:t}$ for a left-to-right tokenizer.
Averaging over $t$ yields \eqref{eq:app_directed_entropy_gap}.
\end{proof}

\subsection{Implications for detection: information bounds}
\label{app:detection-bounds}

\begin{proposition}[Excess Bayes error is controlled by conditional entropy]
\label{prop:fano}
Consider $K$-class classification with semantic label $Y\in\{1,\dots,K\}$.
Let $\mathcal{E}^\star_X$ and $\mathcal{E}^\star_Z$ denote the Bayes classification
errors achievable when observing $X$ and $Z$ respectively under $P^\star$.
Then $H^\star(Y\mid Z)\ge H^\star(Y\mid X)$ and
\begin{equation}
\mathcal{E}^\star_Z
\ \ge\
\frac{H^\star(Y\mid Z)-1}{\log K}.
\tag{C.1}\label{eq:app_fano}
\end{equation}
Moreover, combining with Lemma~\ref{lem:dist-identity},
\begin{equation}
H^\star(Y\mid Z) = H^\star(Y\mid X) + \Delta(\tau),
\tag{C.2}\label{eq:app_entropy_dist}
\end{equation}
so controlling $\Delta(\tau)$ directly controls the increase in label uncertainty
induced by tokenization.
\end{proposition}

\begin{proof}
Since $Z=\tau(X)$ is a function of $X$, conditioning on $Z$ cannot reduce uncertainty
below conditioning on $X$, so $H^\star(Y\mid Z)\ge H^\star(Y\mid X)$.
The bound \eqref{eq:app_fano} follows from Fano's inequality.
Equation \eqref{eq:app_entropy_dist} is Lemma~\ref{lem:dist-identity}.
\end{proof}

\subsection{Probe-based estimation: variational upper bounds}
\label{app:probe}

\begin{proposition}[Any probe yields an upper bound]
\label{prop:upperbound}
For any predictor $q\in\mathcal{Q}$,
\begin{equation}
\Delta(\tau)
\le
\mathbb{E}\left[\mathrm{KL}\!\left(P^\star(Y\mid X)\ \Vert\ q(Y\mid Z)\right)\right],
\qquad
\Delta_{\rightarrow}(\tau)
\le
\mathbb{E}\Big[\tfrac{1}{T}\sum_{t=1}^T \mathrm{KL}(P^\star(Y_t\mid X_{1:t})\Vert q(Y_t\mid Z_{1:t}))\Big].
\tag{D.1}\label{eq:app_upperbound}
\end{equation}
\end{proposition}

\begin{proof}
Immediate from the definition as a minimum over $q$.
\end{proof}

\subsection{Greedy vocabulary induction as marginal Lagrangian improvement}
\label{app:greedy}

We optimize the Lagrangian objective
\begin{equation}
\mathcal{L}(\tau) \triangleq R(\tau) + \lambda\,\Delta_{\rightarrow}(\tau).
\tag{E.1}\label{eq:app_lagrangian}
\end{equation}
Let $\mathcal{V}$ be the current vocabulary and let $\tau_{\mathcal{V}}$ denote the
deterministic tokenizer induced by $\mathcal{V}$ under a fixed segmentation rule
(e.g., longest-match with a deterministic tie-break).
Consider adding a candidate token $c$ to obtain $\mathcal{V}'=\mathcal{V}\cup\{c\}$.
Define the marginal changes:
\[
g(c) \triangleq R(\tau_{\mathcal{V}}) - R(\tau_{\mathcal{V}'}),\qquad
\delta(c) \triangleq \Delta_{\rightarrow}(\tau_{\mathcal{V}'})-\Delta_{\rightarrow}(\tau_{\mathcal{V}}).
\]
Then the Lagrangian change is
\begin{equation}
\mathcal{L}(\tau_{\mathcal{V}'}) - \mathcal{L}(\tau_{\mathcal{V}})
=
- g(c) + \lambda\,\delta(c).
\tag{E.2}\label{eq:app_deltaL}
\end{equation}

\subsection{Deterministic compilation and locality of boundary effects}
\label{app:det-local}

\paragraph{Determinism via compiled matching.}
Fix a vocabulary $\mathcal{V}$ and a deterministic segmentation rule (e.g., left-to-right
longest-match; ties broken by token ID).
Then $\tau_{\mathcal{V}}$ defines a unique mapping $X\mapsto Z$.
Compiling $\mathcal{V}$ into a deterministic matcher (e.g., a deterministic automaton or
WFST) ensures that runtime tokenization is reproducible across environments and has
predictable worst-case behavior.

\paragraph{Linear-time runtime.}
For input length $n$, a deterministic left-to-right matcher processes each character a
constant number of times and emits tokens without backtracking.
Hence runtime is $\mathcal{O}(n)$ with constants determined by the compiled artifact.
This guarantee is independent of input distribution.

\paragraph{Locality and boundary sensitivity.}
Let $\mathcal{B}$ be a set of hard boundaries (e.g., delimiters, whitespace/newline).
Assume a segmentation rule that never produces tokens crossing these boundaries.
Then edits within a single boundary span cannot change tokenization outside a small
neighborhood.

\begin{lemma}[Locality under boundary-respecting tokenization]
\label{lem:locality}
Assume the tokenizer never emits tokens that cross boundaries in $\mathcal{B}$, and
segmentation is left-to-right deterministic.
Let $X$ and $\tilde X$ differ only within one boundary span. Then $\tau(X)$ and
$\tau(\tilde X)$ are identical outside that span.
\end{lemma}

\begin{proof}
By assumption, token boundaries are aligned to boundary spans: tokens are segmented
independently within each span, and spans are processed sequentially.
If $X$ and $\tilde X$ are identical outside one span, the matcher state and emitted
tokens are identical for all preceding spans.
Processing the modified span may change tokens within it, but since no token crosses
into neighboring spans, subsequent spans see the same input and the same initial state,
so emitted tokens are identical thereafter.
\end{proof}

\subsection{Formal stability metrics for boundary sensitivity}
\label{app:stability}

Let $\mathcal{T}$ be a family of semantics-preserving transformations on strings.
For each $x$, define a perturbed sample $\tilde x = T(x)$ with $T\sim\mathcal{T}$.

Define tokenization instability as the expected token-edit distance between token
sequences:
\begin{equation}
\mathrm{Stab}(\tau)
\triangleq
\mathbb{E}_{X}\mathbb{E}_{T\sim\mathcal{T}}
\big[ d_{\mathrm{edit}}(\tau(X),\tau(T(X)))\big].
\tag{J.1}\label{eq:app_stab_def}
\end{equation}

Assume boundary-respecting segmentation with spans induced by $\mathcal{B}$.

\begin{proposition}[Span-local Lipschitz bound]
\label{prop:span_lip}
Let $X$ and $\tilde X$ differ only within $k$ boundary spans.
Then
\begin{equation}
d_{\mathrm{edit}}(\tau(X),\tau(\tilde X))
\le
\sum_{s\in\mathcal{S}}
\left(|\tau(X^{(s)})| + |\tau(\tilde X^{(s)})|\right),
\tag{J.2}\label{eq:app_span_lip}
\end{equation}
where $\mathcal{S}$ is the set of modified spans and $X^{(s)}$ denotes the substring in
span $s$.
\end{proposition}

\begin{proof}
Token sequences outside modified spans are identical and cancel in the edit distance.
Within each modified span $s$, worst-case edit distance is at most the sum of their
lengths, yielding \eqref{eq:app_span_lip}.
\end{proof}

\subsection{Greedy induction with approximation guarantees via a submodular surrogate}
\label{app:submodular}

Algorithm~1 greedily selects tokens by maximizing a gain--distortion score.
The exact objective can exhibit interactions due to overlaps and competition.
Here we introduce a tight surrogate that is monotone submodular and admits classical
greedy approximation guarantees.

\subsubsection{A weighted-coverage surrogate for compression gain}

Fix a corpus $\mathcal{D}=\{x_i\}_{i=1}^N$ and candidate set $\mathcal{C}$.
For each candidate token $c\in\mathcal{C}$, define $\mathrm{Occ}(c)$ as the set of its
occurrences in the corpus at the character/span level.
For each occurrence $o\in \mathrm{Occ}(c)$, assign weight $w(o)\ge 0$ representing
potential token-count reduction.

For selected set $S\subseteq \mathcal{C}$, define surrogate gain
\begin{equation}
G(S)
\triangleq
\sum_{o\in \Omega} \max_{c\in S:\ o\in \mathrm{Occ}(c)} w(o),
\tag{K.1}\label{eq:app_submod_gain}
\end{equation}
where $\Omega = \bigcup_{c\in\mathcal{C}}\mathrm{Occ}(c)$.

\begin{lemma}[Monotone submodularity of the coverage surrogate]
\label{lem:submod}
$G(S)$ is normalized, monotone nondecreasing, and submodular.
\end{lemma}

\begin{proof}
Normalization is immediate.
Monotonicity holds because adding a token can only increase the max for each occurrence.
Submodularity follows by diminishing returns: for $A\subseteq B$ and $c\notin B$, the
marginal improvement is pointwise larger for $A$ than for $B$; summing yields the claim.
\end{proof}

\subsection{Directed semantic distortion as irreducible online prediction regret}
\label{app:regret}

Consider sequential targets $Y_{1:T}$ and inputs $X_{1:T}$.
Under log-loss, the Bayes-optimal expected loss equals conditional entropy.

\begin{theorem}[Directed distortion as irreducible regret]
\label{thm:dist_regret}
Assume the student predictor class is unrestricted (or rich enough to represent
$P^\star(Y_t\mid Z_{1:t})$).
Then the minimum achievable expected cumulative excess log-loss of observing tokenized
prefixes instead of raw prefixes is
\begin{equation}
\sum_{t=1}^T \Big(H^\star(Y_t\mid Z_{1:t}) - H^\star(Y_t\mid X_{1:t})\Big)
=
T\,\Delta_{\rightarrow}(\tau).
\tag{L.3}\label{eq:app_regret_equals}
\end{equation}
\end{theorem}

\begin{proof}
Immediate from Lemma~\ref{lem:directed-identity}.
\end{proof}

